{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYMtReqyRybc"
      },
      "source": [
        "## Python II Training Session\n",
        "### Introduction to Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQVHchJ3SDfB"
      },
      "source": [
        "This notebook contains content related to Python II training session. We will cover the following topics:\n",
        "\n",
        "1. [Creating and recoding variables in Pandas](#1)\n",
        "2. [Using Pingouin and SciPy libraries for inferential statistics](#2)\n",
        "3. [Visualizations](#3)\n",
        "4. [Handling output](#4)\n",
        "5. [Advanced Topics and Examples](#5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<strong><span style=\"color: #ff0000;\">NOTE</span>:</strong> **This training session is conducted on Google Colab. The instructions below assume that you are viewing this notebook directly on Colab.**<br><br>\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/citl-data/training_session_data/blob/main/Python_II_Training.ipynb)<br><br>\n",
        "If you wish to use this notebook locally, ensure that you are running at least Python 3.11 and all the necessary package dependencies are met. **You can skip some of the setup code cells that are marked \"Colab Only\" at the beginning of the cell.** You may also need to modify the path to the dataset file to the appropriate location on your local machine.<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCRkX7PoQtRr"
      },
      "source": [
        "**Jupyter Notebooks**\n",
        "\n",
        "Earlier called the IPython notebook (hence the `.ipynb` extension), Jupyter notebooks are an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. They are widely used for data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, etc.\n",
        "\n",
        "**Google Colaboratory** (Colab in short) is built on top of Jupyter notebooks and has added functionalities that are found in more conventional IDEs like PyCharm, RStudio, Spyder, etc.\n",
        "\n",
        "Colab is entirely cloud-based and the free-tier even allows for some free GPU and TPU processing capabilities. Additionally, it can connect with your Google Drive storage for accessing and storing data.\n",
        "\n",
        "In case you want to install Python and Jupyter notebooks on your local machine, [follow this](https://docs.anaconda.com/free/anaconda/install/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhuuoL_7U0N6"
      },
      "source": [
        "**Common shortcuts**\n",
        "\n",
        "Jupyter Notebook cells have 2 modes, 'Command' and 'Edit'.\n",
        "\n",
        "To switch from command to edit mode press `Enter`. To switch from edit to command mode press `Esc`.\n",
        "\n",
        "Execute cell: `Ctrl+Enter`\n",
        "Execute cell and move to next cell: `Shift+Enter` (This will add a new cell below if none exists)\n",
        "Execute cell and add new cell below: `Alt+Enter`\n",
        "\n",
        "In command mode:\n",
        "- insert cell above: `a`\n",
        "- insert cell below: `b`\n",
        "- change cell type to Markdown: `m` (`Ctrl+mm` in Colab)\n",
        "- change cell type to code: `y` (`Ctrl+my` in Colab)\n",
        "- delete cell: `dd` (`Ctrl+md` in Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27KI5ef-n7H3"
      },
      "source": [
        "# [0] Setup\n",
        "Run the cell below to mount your Google Drive folder and create a copy of the training materials. You will need to provide permission to Colab for accessing your Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG2yvwE8sXYe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Magic Commands**\n",
        "\n",
        "Jupyter notebooks allow you to execute certain operations outside of the Python environment. Often, we might need to directly access the system terminal for installing new packages and make other changes outside the environment. These are called magic commands and begin with `!`.\n",
        "\n",
        "You can even turn an entire cell to access an outside environment by specifying it with the `%%` in the first line. Every subsequent command within the cell will refer to the external environment and not Python (without using `!`). Here, we are going to use the `bash` shell to download the dataset used for this workshop and save it in your Drive folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Colab only###\n",
        "\n",
        "%%bash\n",
        "mkdir /gdrive/MyDrive/Colab\\ Notebooks/pyworkshop # Creates new folder called pyworkshop\n",
        "ls /gdrive/MyDrive/Colab\\ Notebooks/ -l # Check if the new folder was created"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dowoad a copy of the dataset file used for this training session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/citl-data/training_session_data/main/GSS2018.csv'\n",
        "target_dir = '/gdrive/MyDrive/Colab Notebooks/pyworkshop' # Modify path if runnning locally\n",
        "file_path = os.path.join(target_dir, 'GSS2018.csv')\n",
        "\n",
        "response = requests.get(url)\n",
        "with open(file_path, 'wb') as f:\n",
        "    f.write(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObV88WzltXIe"
      },
      "source": [
        "# [1] Working With DataFrames: Data transformation <a name=\"1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcBKo-zVtgqu"
      },
      "source": [
        "A majority of data analysis in Python will involve using the `pandas` library to deal with dataframes. It provides similar functionalities to R dataframes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLbzKTbpupG9"
      },
      "source": [
        "Let's first import the GSS 2018 data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnQWAKj9unjh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "my_data = pd.read_csv('/gdrive/MyDrive/Colab Notebooks/pyworkshop/GSS2018.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSlXwOWxvFD6"
      },
      "source": [
        "While working with data sets, we often need to create new variables or recode the current variable into new values. This can also be easily done with the help of `pandas`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KFAwZ8UvINL"
      },
      "source": [
        "## [a] Creating a new variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-QLCou-vKNA"
      },
      "source": [
        "To create a new variable in Python, you can directly use `varname = ` to specify this new variable.\n",
        "\n",
        "But to create a new variable in a dataframe, you have to use the object name and the dataframe to specify the name of the created variable.\n",
        "\n",
        "For instance, you need to type `my_data['birth_year'] = ` rather than `'birth_year' = ` so that the system understands the newly created variable is an element of `my_data` rather than a stand-alone variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzTND64lvi0L"
      },
      "source": [
        "Let's compute people's `birth_year` based on their `age`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYLcylqXvGAg"
      },
      "outputs": [],
      "source": [
        "my_data['birth_year'] = 2018 - my_data['age']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGSzhy-GvB26"
      },
      "outputs": [],
      "source": [
        "# use describe() to see the descriptive statistics\n",
        "my_data['birth_year'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dny935aurw7"
      },
      "source": [
        "You can also check whether the new variable has been created successfully by previewing the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oFIwdFOorP_"
      },
      "outputs": [],
      "source": [
        "# Display all the columns\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU4xbggFuxQ6"
      },
      "outputs": [],
      "source": [
        "my_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD1iMUcRwIHp"
      },
      "source": [
        "## [b] Recoding variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vSjqAvFwLRo"
      },
      "source": [
        "Recoding variables with `pandas` is simple.\n",
        "\n",
        "The recode procedure transforms an existing variable into a new variable by changing, rearranging, or consolidating the values of the existing variable. It allows the user to:\n",
        "\n",
        "* Change an existing categorical variable into a new categorical variable with a smaller number of categories/levels.\n",
        "* Change an existing continuous variable with many unique values (e.g, `income`, `age`, etc.)  into a categorical variable with fewer levels (for example, if recoded, the variable income will have the values low, medium, and high, while the variable age will have young, adult, and senior as values).\n",
        "* Code missing values.\n",
        "* Replace miscoded values with correct values.\n",
        "* Create a dummy variable based on a cutoff value.\n",
        "\n",
        "To recode a variable, one or more of Python’s logical operators are used. You may refer to **Python I Notebook** for common logical operators.\n",
        "\n",
        "For instance, if you want to recode `age` based on the following rule:\n",
        "\n",
        "1. `age` <= 21: `non-adult`;\n",
        "2. `age` > 21: `adult`\n",
        "\n",
        "You can use the `np.where` function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvA3pTGzwffp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "my_data['age_group'] = np.where(my_data['age'] > 21, \"adult\", 'non-adult')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvLt_Ojjv6yZ"
      },
      "outputs": [],
      "source": [
        "# You can use value_counts() to get the frequency of each value\n",
        "my_data['age_group'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIzM8Lx8yV0H"
      },
      "source": [
        "If the original variable is categorical, we can use `replace` function to recode the values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkRUmIAiyoVd"
      },
      "source": [
        "For example, let's check the variable 'sex'. Let's recode it using the following rule:\n",
        "\n",
        "* `1` --> `'Male'`\n",
        "* `2` --> `'Female'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kmy_OEXcvAsX"
      },
      "outputs": [],
      "source": [
        "my_data['sex'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gh2eotrtb9A"
      },
      "outputs": [],
      "source": [
        "my_data['sex'] = my_data['sex'].replace({1:\"Male\", 2:\"Female\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc_LZneozaxh"
      },
      "outputs": [],
      "source": [
        "my_data['sex'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR_LPNCjorPy"
      },
      "source": [
        "### Exercise 1\n",
        "Recode `marital` using the following rule:\n",
        "\n",
        "* 1: 'MARRIED',\n",
        "* 2: 'WIDOWED',\n",
        "* 3: 'DIVORCED',\n",
        "* 4: 'SEPARATED',\n",
        "* 5: 'NEVER MARRIED',\n",
        "* 9: 'NA'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5SM4lpuorPy"
      },
      "outputs": [],
      "source": [
        "my_data['marital_labeled'] = my_data['marital'].replace({1:\"MARRIED\",\n",
        "                                                         2:\"WIDOWED\",\n",
        "                                                         3:\"DIVORCED\",\n",
        "                                                         4:\"SEPARATED\",\n",
        "                                                         5:\"NEVER MARRIED\",\n",
        "                                                         9:np.nan})\n",
        "# Note: notice how we recode 9 into missing values here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_3hxLVXorPy"
      },
      "outputs": [],
      "source": [
        "# Use isna() to detect where a value is missing and then sum() to get total number of missing values in the column\n",
        "my_data['marital_labeled'].isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9dZ9yYNorP-"
      },
      "source": [
        "# [2] Inferential Statistics <a name=\"2\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PVLTTW1ZBvJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q pingouin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0H1rzMRZLyR"
      },
      "outputs": [],
      "source": [
        "import pingouin as pg\n",
        "import scipy.stats as stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCXjIOF2orP9"
      },
      "source": [
        "## [a] Contingency table and chi-square test\n",
        "\n",
        "The `crosstab()` function creates a contingency table.  \n",
        "\n",
        "A contingency table is used to show the joint distribution of cases over two or more categorical variables. The basic syntax to produce a contingency table,`mytab = pd.crosstab(var1, var2)`, will result in a table with `var1` in the rows and `var2` in the columns.\n",
        "\n",
        "Let’s create a contingency table using the variables `marital_labeled` and `sex`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C9mTn1SorP-"
      },
      "outputs": [],
      "source": [
        "# Contingency table\n",
        "# A contingency table is used to show the joint...\n",
        "# ...distribution of cases over two or more categorical variables\n",
        "# The basic syntax to produce a contingency table is\n",
        "# mytab = pd.crosstab(var1 (index), var2 (columns))\n",
        "# For example, var1 = marital and var2 = sex\n",
        "\n",
        "mytable = pd.crosstab(my_data[\"sex\"], my_data[\"marital_labeled\"])\n",
        "mytable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9hd70nmorP-"
      },
      "source": [
        "If you wish to see the percentage, you can do it by specifying the value of normalize to `index` or `column` or `all`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOTsFiUyorP-"
      },
      "outputs": [],
      "source": [
        "# To see the percentage\n",
        "mytable_percentage = pd.crosstab(my_data[\"sex\"], my_data[\"marital_labeled\"], normalize = 'all')\n",
        "mytable_percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbODeY2yt-FS"
      },
      "source": [
        "If you would like to run statistical tests and analyses like the `Pearson's χ2 test` on the two variables, we can use the `chi2_independence` function from the `pingouin` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aVHOD_5ZiG7"
      },
      "outputs": [],
      "source": [
        "expected, observed, stat_vals = pg.chi2_independence(my_data, x='sex', y='marital_labeled')\n",
        "stat_vals\n",
        "# print(f'Expected values: {expected}')\n",
        "# print(f'Observed values: {observed}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKP3jljvxzqB"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Find out if there is a significant association between country of birth `born` and speaking a language other than English `othlang`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQghcQmgx5rN"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# Step 1: perform test\n",
        "\n",
        "\n",
        "# Step 2: interpret the result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfYe1RBOorP-"
      },
      "source": [
        "## [b] Independent samples t-test\n",
        "\n",
        "An independent samples t-test determines whether there is a significant difference in the means of a continuous variable between two groups.\n",
        "\n",
        "For this example, we'll test whether there is a significant difference in the responses between men and women `sex` and how much time they have to relax per day `hrlax`\n",
        "\n",
        "Before performing a t-test, we need to compare the variances of two samples. Different t-test methods make different assumptions about whether or not the variances of the two samples differ. To test for equality of variances, we will perform a Levene’s Test using the `stats.levene` function of the `scipy` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxRJ691Da7Hx"
      },
      "outputs": [],
      "source": [
        "lev, p = stats.levene(my_data[my_data['sex']=='Male']['hrlax'],\n",
        "                my_data[my_data['sex']=='Female']['hrlax'], nan_policy='omit')\n",
        "# Notice that we include an addiitonal argument to handle missing data\n",
        "\n",
        "print(f'The p-value of the Levene test is {p}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1je9omBa9Tq"
      },
      "source": [
        "The Levene method tests the null hypothesis that the variances are equal. Because the significance of Levene’s test in this example is larger than 0.05, we fail to reject the null hypothesis, and the assumption of equal variance is met. In such a case, the statistics for equal variances assumed should be used.\n",
        "\n",
        "We'll use the `pg.ttest` function to test whether there is a significant difference in means between the two groups. Notice that it includes a parameter called `correction`. It is recommended to set it to `auto`, which will automatically use Welch T-test when the equal variances assumption is violated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhnAsREhcg2n"
      },
      "outputs": [],
      "source": [
        "# Running an independent samples t-test\n",
        "\n",
        "pg.ttest(my_data[my_data['sex']=='Male']['hrlax'],\n",
        "         my_data[my_data['sex']=='Female']['hrlax'],\n",
        "         correction='auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFKAWzxFyV6r"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Compare average income of male and female using the variable `sex_new` and `realrinc` and interpret the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcCywma2yw5l"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n",
        "# step 1: get the mean of realrinc for both female and male separately\n",
        "\n",
        "\n",
        "\n",
        "# step 2: perform t test\n",
        "\n",
        "\n",
        "\n",
        "# step 3: interpret the result\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1IMzAOYorP-"
      },
      "source": [
        "## [c] ANOVA\n",
        "\n",
        "A one-way ANOVA (analysis of variance) is used to determine whether there is a\n",
        "statistically significant difference in the mean of a continuous dependent variable among more than two groups.\n",
        "\n",
        "For this example, we'll test whether there is a significant difference in means in the hours spent watching TV `tvhours` between different marital statuses `marital_labeled`.\n",
        "\n",
        "We will again need to test for equality of variances among groups using the `stats.levene` function.\n",
        "\n",
        "We can then use either `pg.anova` or `pg.welch_anova` functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwYXdOKrhUBD"
      },
      "outputs": [],
      "source": [
        "lev, p = stats.levene(my_data[my_data['marital_labeled']=='MARRIED']['tvhours'],\n",
        "                      my_data[my_data['marital_labeled']=='WIDOWED']['tvhours'],\n",
        "                      my_data[my_data['marital_labeled']=='DIVORCED']['tvhours'],\n",
        "                      my_data[my_data['marital_labeled']=='SEPARATED']['tvhours'],\n",
        "                      my_data[my_data['marital_labeled']=='NEVER MARRIED']['tvhours'],\n",
        "                      nan_policy='omit')\n",
        "\n",
        "print(f'The p-value of the Levene test is {p}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM1xKkr4hrP6"
      },
      "outputs": [],
      "source": [
        "# Conducting a one-way ANOVA\n",
        "\n",
        "# If the Levene's test was not significant, that is, equal variances are assumed:\n",
        "\n",
        "aov = pg.anova(data=my_data, dv='tvhours', between='marital_labeled')\n",
        "print('One-way ANOVA\\n',aov)\n",
        "\n",
        "\n",
        "# If the Levene's test was significant, that is, equal variances are not assumed:\n",
        "\n",
        "welchaov = pg.welch_anova(data=my_data, dv='tvhours', between='marital_labeled')\n",
        "print('\\n Welch ANOVA\\n',welchaov)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziaq7GIUZB7y"
      },
      "source": [
        "While the ANOVA procedure determines whether differences exist among the group means, post hoc tests are needed to determine which means differ from one another. Which post hoc test you use depends on the homogeneity of the variances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0yoHoHzlnOY"
      },
      "outputs": [],
      "source": [
        "# Conducting post-hoc tests\n",
        "\n",
        "# Tukey's HSD\n",
        "\n",
        "# pg.pairwise_tukey(data=my_data, dv='tvhours', between='marital_labeled').round(3)\n",
        "\n",
        "# Games-Howell test\n",
        "\n",
        "pg.pairwise_gameshowell(data=my_data, dv='tvhours', between='marital_labeled').round(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMnV1fRCzOJC"
      },
      "source": [
        "### Exercise 4\n",
        "Test whether the income differs significantly among different education levels using the variable `degree` and `realrinc`.\n",
        "\n",
        "Then perform ANOVA test and interpret the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLeNRv0ZzuOZ"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n",
        "\n",
        "# step 1: conduct Levene's test\n",
        "\n",
        "\n",
        "\n",
        "# step 2: perform ANOVA\n",
        "\n",
        "\n",
        "\n",
        "# step 3: interpret the result\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RWGuRUQy7vW"
      },
      "source": [
        "## [d] Correlation Matrix\n",
        "\n",
        "Correlation is a measure of the linear relationship between two continuous variables. A correlation coefficient ranges from -1 to 1. A positive correlation coefficient indicates a positive linear relationship (i.e., as one variable increases, the other tends to as well). On the other hand, a negative correlation coefficient indicates a negative linear relationship. A correlation coefficient of 0 indicates there is no linear relationship between the two variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ-AbfVVorP_"
      },
      "outputs": [],
      "source": [
        "# Get the correlation matrix between age, realinc, hrs1, and tvhours\n",
        "\n",
        "my_data[['age','realrinc','hrs1','tvhours']].corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udr3tCdPoCFu"
      },
      "outputs": [],
      "source": [
        "# Correlation matrix with significance levels\n",
        "\n",
        "my_data[['age','realrinc','hrs1','tvhours']].rcorr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIh3S9MznPlg"
      },
      "source": [
        "## [e] Linear Regression\n",
        "\n",
        "Linear regression is used to assess the association between one or more independent variables (continuous or categorical) and a continuous dependent variable.\n",
        "\n",
        "We'll use the `pg.linear_regression` function to fit a linear regression model to test the association between the family income `realrinc` and other independent variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnPzyShXnVq6"
      },
      "outputs": [],
      "source": [
        "# Simple or bivariate regression with age as the only IV\n",
        "\n",
        "lm1 = pg.linear_regression(my_data['age'], my_data['realrinc'], remove_na=True)\n",
        "lm1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GoOEuynpf7V"
      },
      "outputs": [],
      "source": [
        "# Multiple linear regression with age and occupational prestige score as IVs\n",
        "\n",
        "lm2 = pg.linear_regression(my_data[['age','pres']], my_data['realrinc'], remove_na=True)\n",
        "lm2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es63d8PtorP_"
      },
      "source": [
        "# [3] Visualizations <a name=\"3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiMHBGW-orP_"
      },
      "source": [
        "## [a] Histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSlWrkUWorP_"
      },
      "source": [
        "A histogram visually displays the distribution of a continuous variable. In Python, the `plot.hist` function works as shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM67pHqTorQA"
      },
      "outputs": [],
      "source": [
        "# Graphics\n",
        "# Histogram\n",
        "my_data['age'].plot.hist(title = 'Age Distribution',\n",
        "                         color = 'green',\n",
        "                         bins = 20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_C7AJQMorQA"
      },
      "source": [
        "## [b] Bar graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X9tv4G4orQA"
      },
      "source": [
        "A bar graph can display the frequencies or distribution of a categorical variable. First, a data table must be created for plotting. In this case, we name our data table sexcount, then plot the bar graph of the `sex` variable by using the `plot.bar`function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd4xfz1iorQA"
      },
      "outputs": [],
      "source": [
        "# Bar Graph\n",
        "sexcount = my_data['sex'].value_counts()\n",
        "sexcount.plot.bar(title = 'Sex counts',\n",
        "                 color = 'blue')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAuQW1y8orQA"
      },
      "source": [
        "A grouped bar graph is a bar graph that uses an additional variable to group the plotted data. For example, you can create a grouped bar graph displaying sex and marital status. If you would like to plot the grouped bar plot of sex and marital variables, you will need to first use the crosstab command to get the frequencies and then use the `plot.bar` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kZvc0RiorQA"
      },
      "outputs": [],
      "source": [
        "# Grounded bar graph\n",
        "groupbar = pd.crosstab(my_data['sex'], my_data['marital'])\n",
        "\n",
        "groupbar.plot.bar(title = 'Sex vs. Marital',\n",
        "                 color = ['sandybrown','salmon','skyblue','teal','orchid','cornflowerblue'])\n",
        "\n",
        "\n",
        "# Named colors https://matplotlib.org/stable/gallery/color/named_colors.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrrjFwuhorQA"
      },
      "source": [
        "### Exercise 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni98BLPporQA"
      },
      "source": [
        "Create a grouped bar chart to show the number of respondents across the variable `degree` and `sex`. The x is degree, y is the frequency and the group is `sex`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpjsYHtk0Uue"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n",
        "# step 1: get the crosstable\n",
        "\n",
        "\n",
        "\n",
        "# step 2: plot the grouped bar chart\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7--qlwVorQA"
      },
      "source": [
        "## [c] Scatterplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYBeLTuKorQA"
      },
      "source": [
        "A scatterplot plots the “positions” of two variables in dimensions x and y. Let's plot the the relationship between age and weekly working hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKzyq8FtorQA"
      },
      "outputs": [],
      "source": [
        "# Scatterplot\n",
        "my_data.plot.scatter(x = 'age', y = 'hrs1',\n",
        "                     title = 'The relationship between age and the weekly working hours')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caHIDi6horQA"
      },
      "source": [
        "## [d] Using Matplotlib library for advanced visualizations\n",
        "When it comes to more complex visualizations, or when you want to customize your plots, you can use the Matplotlib library. Matplotlib is a comprehensive library for creating static, animated, and interactive plots in Python. It is a powerful library that can be used to create a wide range of plots, including histograms,scatter plots, bar charts, pie charts, and more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMofbYS9orQB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eIB249v03Qq"
      },
      "source": [
        "### [i] Scatterplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NAnQ1X7orQB"
      },
      "outputs": [],
      "source": [
        "# create a scatter plot where x being age and y being hrs1\n",
        "plt.scatter(my_data['age'], my_data['hrs1'], color = 'blue')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbC6dAZUorQB"
      },
      "outputs": [],
      "source": [
        "# create a scatter plot where x being age and y being hrs1, also differentiating the color by gender\n",
        "colors = {'Male': 'blue', 'Female': 'red'}\n",
        "\n",
        "plt.scatter(my_data['age'], my_data['hrs1'],\n",
        "            c=my_data['sex'].map(colors), label=my_data['sex'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJyk2B7U0_l9"
      },
      "source": [
        "### [ii] Line chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H3-OmGworQB"
      },
      "outputs": [],
      "source": [
        "# group by age and calculate the mean of hrs1 and mean of sphrs1\n",
        "hr_by_age = my_data.groupby('age').agg({'hrs1':'mean'}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcewFMLCorQB"
      },
      "outputs": [],
      "source": [
        "plt.plot(hr_by_age['age'], hr_by_age['hrs1'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cioObNyw2QU1"
      },
      "source": [
        "### [iii] Multiple charts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMheoSsY2xVm"
      },
      "outputs": [],
      "source": [
        "hr_by_age = my_data.groupby('age').agg({'hrs1':'mean','sphrs1':'mean'}).reset_index()\n",
        "hr_by_age = hr_by_age[hr_by_age['age'] != 99]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHwb5Ee6orQB"
      },
      "outputs": [],
      "source": [
        "plt.plot(hr_by_age['age'], hr_by_age['hrs1'], label = 'hrs1')\n",
        "# add title\n",
        "plt.title('The relationship between age and the weekly working hours')\n",
        "# add x and y axis labels\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Average Weekly working hours')\n",
        "# add points\n",
        "plt.scatter(hr_by_age['age'], hr_by_age['hrs1'], color = 'blue')\n",
        "# add a new line for sphrs1 using dotted line\n",
        "plt.plot(hr_by_age['age'], hr_by_age['sphrs1'], label = 'sphrs1', color = 'red', linestyle = '--')\n",
        "# add points for sphrs1 using a different triangle marker\n",
        "plt.scatter(hr_by_age['age'], hr_by_age['sphrs1'], color = 'red', marker = '^')\n",
        "# add legend by color\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC8FgF8lorQB"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.subplot(211) # plt.subplot(nmk) where n is the number of rows, m is the number of columns, and k is the plot number\n",
        "plt.plot(hr_by_age['age'], hr_by_age['hrs1'], label = 'hrs1')\n",
        "# add title\n",
        "plt.title('Self weekly working hours')\n",
        "# add x and y axis labels\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Average Weekly working hours')\n",
        "# add points\n",
        "plt.scatter(hr_by_age['age'], hr_by_age['hrs1'], color = 'blue')\n",
        "# add some blank space between the two plots\n",
        "plt.subplots_adjust(hspace = 0.5)\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.plot(hr_by_age['age'], hr_by_age['sphrs1'], label = 'sphrs1', color = 'red', linestyle = '--')\n",
        "plt.title('Spouse weekly working hours')\n",
        "# add x and y axis labels\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Average Weekly working hours')\n",
        "# add points for sphrs1 using a different triangle marker\n",
        "plt.scatter(hr_by_age['age'], hr_by_age['sphrs1'], color = 'red', marker = '^')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6SJ_ion2sxw"
      },
      "outputs": [],
      "source": [
        "income_by_age_gender = my_data.groupby(['age','sex']).agg({'realrinc':'mean'}).reset_index()\n",
        "income_by_age_gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irw9sqPG3TZ7"
      },
      "outputs": [],
      "source": [
        "income_by_age_male = income_by_age_gender[income_by_age_gender['sex'] == 'Male']\n",
        "income_by_age_female = income_by_age_gender[income_by_age_gender['sex'] == 'Female']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn2oPLbW3N5D"
      },
      "outputs": [],
      "source": [
        "plt.plot(income_by_age_male['age'], income_by_age_male['realrinc'], label = 'Male')\n",
        "# add title\n",
        "plt.title('The relationship between age and the income by different sex')\n",
        "# add x and y axis labels\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Income')\n",
        "# add points\n",
        "plt.scatter(income_by_age_male['age'], income_by_age_male['realrinc'], color = 'blue')\n",
        "# add a new line for sphrs1 using dotted line\n",
        "plt.plot(income_by_age_female['age'], income_by_age_female['realrinc'], color = 'red', linestyle = '--', label = 'Female')\n",
        "# add points for sphrs1 using a different triangle marker\n",
        "plt.scatter(income_by_age_female['age'], income_by_age_female['realrinc'], color = 'red', marker = '^')\n",
        "# add legend by color\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoaow0sqorQC"
      },
      "source": [
        "#[4] Saving and exporting <a name=\"4\"></a>\n",
        "\n",
        "If you wish to export the data to `.csv` file, you may follow `df.to_csv (export_file_path, index = False, header=True)` to save your data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls2md59Q4qcL"
      },
      "outputs": [],
      "source": [
        "my_data.to_csv('exportmydata.csv', index = False, header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmJmWoU033JQ"
      },
      "source": [
        "#[5] Advanced Topics and Examples <a name=\"5\"></a>\n",
        "\n",
        "While Python can handle most common statistical analyses, it is first and foremost a general purpose programming language. To truly leverage the unique advantages and flexibility of Python, knowledge of some additional programming concepts is necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_wYBddk6Y-9"
      },
      "source": [
        "##[a] Control Flow - Loops\n",
        "\n",
        "We often need to execute a set of commands/operations repeatedly. Instead of writing out the same set of code multiple times, we can encapsulate the piece of code within a looping code block.\n",
        "\n",
        "There are 2 types of loops in Python.\n",
        "\n",
        "The `for` loop is used when the number of iterations is predetermined and fixed before the looping block is executed. The loop ends when the last iteration is completed.\n",
        "\n",
        "The `while` loop is used when the number of iterations is not predetermined. The loop ends when a specific condition is met. Hence, the number of iterations is not fixed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCD89mMa_IPx"
      },
      "outputs": [],
      "source": [
        "a_num = [2,4,6,2,8,2,6,7,3,1]\n",
        "\n",
        "for i in a_num:\n",
        "  print(i)\n",
        "\n",
        "for i in range(10):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3ps9A8k_SIC"
      },
      "outputs": [],
      "source": [
        "c = 0\n",
        "while c<len(a_num):\n",
        "  print(a_num[c])\n",
        "  c+=1\n",
        "\n",
        "\n",
        "# Sometimes it may be necessary to enter the loop at least once and check the\n",
        "# exit condition at the very end instead of the beginning\n",
        "c = 0\n",
        "while True:\n",
        "  print(a_num[c])\n",
        "  c+=1\n",
        "  if c>=len(a_num): break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpvsbWmPorPz"
      },
      "source": [
        "##[b] Functions\n",
        "\n",
        "Functions are a set of well-defined and self-contained instructions that accomplishes a particular task. These are smaller modules of code that can be invoked from anywhere within the rest of programming code. Any Python command that is followed by `()` is a function.\n",
        "\n",
        "Python functions start with the keyword `def` which means you are defining your own functions.\n",
        "\n",
        "Next, it is followed by the **function name** and a set of **parentheses**, within which the function inputs are specified. After the inputs are specified, you go in to the function body. Note that the **indentation** always matters and your function body cannot be aligned with the keyword `def`. At the end of your function, if you need your function to return some outputs, you also need to specify what you want it to `return`. Functions in Python usually look something like this:\n",
        "</br></br>\n",
        "\n",
        "```\n",
        "def FunctionName(input1, input2, input3):\n",
        "\n",
        "    FunctionConditions\n",
        "\n",
        "    return output\n",
        "```\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2akAV1p2orPz"
      },
      "source": [
        "Now let’s look at an example of recoding the `age` variable into three categories, `Young`, `Middle Aged`, and `Elder`. To create this new variable, we only need to know the original numeric age values.\n",
        "\n",
        "Now, we create our own fuction called `transform_age` which takes the numeric value of `age` as the function input. Within this function, the function body is a series of conditional statements of our coding schema. Since the recode variable is a string variable, we ask this `transform_age` function to return the corresponding categories.\n",
        "\n",
        "Once we have this function, we can use the `apply` command to apply this function to the age variable and get what we want. We can use the `unique()` command to check that our transformation is indeed what we hoped for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t7HzwbDorPz"
      },
      "outputs": [],
      "source": [
        "# When we have more than 2 categories (e.g., age)\n",
        "# We will need to write a function to implement\n",
        "\n",
        "def recode_age(age):\n",
        "    if age > 75:\n",
        "        return 'Elder'\n",
        "    elif age >45:\n",
        "        return 'Middle Aged'\n",
        "    else:\n",
        "        return 'Young'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnTuDsFhorPz"
      },
      "outputs": [],
      "source": [
        "# Check if function works\n",
        "print(recode_age(80))\n",
        "print(recode_age(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErMs8Uq-orPz"
      },
      "outputs": [],
      "source": [
        "# Add a new column for the new categories\n",
        "my_data['age_cat'] = my_data['age'].apply(recode_age)\n",
        "my_data['age_cat']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-SSa-u9sMhe"
      },
      "outputs": [],
      "source": [
        "# get the unique values in the column\n",
        "my_data['age_cat'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI2Y5XbDorP9"
      },
      "outputs": [],
      "source": [
        "# Count frequencies\n",
        "my_data['age_cat'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8VeqvdFBrBR"
      },
      "source": [
        "##[c] Examples\n",
        "\n",
        "The topics covered across Python I and Python II cover most of the fundamental concepts, commands, and data structures that underlie **every** Python module, library, or program.\n",
        "\n",
        "Let us look at an examples of how we can combine all of these concepts to accomplish more complex tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic-vibwnGqSS"
      },
      "source": [
        "### Scraping Wikipedia pages and generating wordclouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuvrmlsEPlXF"
      },
      "outputs": [],
      "source": [
        "# Load necessary libraries\n",
        "# Corresponding documentation websites provided\n",
        "import requests # https://requests.readthedocs.io/en/latest/\n",
        "from bs4 import BeautifulSoup as bs # https://beautiful-soup-4.readthedocs.io/en/latest/\n",
        "from wordcloud import WordCloud, STOPWORDS # https://amueller.github.io/word_cloud/index.html\n",
        "import matplotlib.pyplot as plt # https://matplotlib.org/stable/index.html\n",
        "from nltk.corpus import stopwords\n",
        "import nltk # https://www.nltk.org/\n",
        "import re # https://docs.python.org/3/library/re.html\n",
        "import pandas as pd # https://pandas.pydata.org/docs/\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXmx5LmJPmzW"
      },
      "outputs": [],
      "source": [
        "# Function to get text from a Wikipedia page\n",
        "def get_wiki_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = bs(response.text, 'html.parser')\n",
        "    paragraphs = soup.find_all('p')\n",
        "    text = ' '.join([p.text for p in paragraphs])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea1Zik_SP-g1"
      },
      "outputs": [],
      "source": [
        "languages = [\n",
        "    'Python_(programming_language)',\n",
        "    'R_(programming_language)',\n",
        "    'JavaScript',\n",
        "    'Java_(programming_language)'\n",
        "]\n",
        "\n",
        "language_word_counts = {}\n",
        "language_texts = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vLrQ5yRQNtG"
      },
      "outputs": [],
      "source": [
        "for language in languages:\n",
        "    # Create the Wikipedia URL\n",
        "    url = f'https://en.wikipedia.org/wiki/{language}'\n",
        "\n",
        "    # Get the text content\n",
        "    text = get_wiki_text(url)\n",
        "    language_texts[language] = text\n",
        "\n",
        "    # Clean text - remove numbers and special characters\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Count unique words that are at least 3-letters long\n",
        "    words = cleaned_text.lower().split()\n",
        "    words = list(set(words))\n",
        "    word_count = 0\n",
        "    for word in words:\n",
        "        if len(word) > 3 and word not in stopwords.words('english'):\n",
        "            word_count += 1\n",
        "\n",
        "    # Store the word count\n",
        "    language_word_counts[language] = word_count\n",
        "    print(f\"{language.replace('_(programming_language)', '')}: {word_count} words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUDENzp4QpOs"
      },
      "outputs": [],
      "source": [
        "# Function to generate and display wordcloud\n",
        "def generate_wordcloud(text, title):\n",
        "    combined_stopwords = STOPWORDS.union(set(stopwords.words('english')))\n",
        "    wordcloud = WordCloud(\n",
        "        stopwords=combined_stopwords,\n",
        "        background_color='white',\n",
        "        max_words=100,\n",
        "        width=800,\n",
        "        height=400\n",
        "    ).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BHfoB0GQ3cd"
      },
      "outputs": [],
      "source": [
        "# Generate word clouds for each language\n",
        "for language, text in language_texts.items():\n",
        "    display_name = language.replace('_(programming_language)', '')\n",
        "    generate_wordcloud(text, f\"Word Cloud for {display_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "752j-PkbQjFI"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create a simple contingency table for two words - programming and object\n",
        "words_to_check = [\"programming\", \"object\"]\n",
        "contingency_table = []\n",
        "\n",
        "for language in languages:\n",
        "    # Clean text and convert to lowercase\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', language_texts[language]).lower()\n",
        "\n",
        "    # Count occurrences of each word\n",
        "    programming_count = cleaned_text.count(\" programming \")\n",
        "    object_count = cleaned_text.count(\" object \")\n",
        "\n",
        "    # Add to contingency table\n",
        "    contingency_table.append([programming_count, object_count])\n",
        "\n",
        "# Create a readable DataFrame for display\n",
        "language_names = [lang.replace('_(programming_language)', '') for lang in languages]\n",
        "contingency_df = pd.DataFrame(contingency_table,\n",
        "                             index=language_names,\n",
        "                             columns=words_to_check)\n",
        "\n",
        "print(\"Contingency Table:\")\n",
        "print(contingency_df)\n",
        "\n",
        "# Perform chi-square test using scipy library\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"\\nChi-square Independence Test Results:\")\n",
        "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
        "print(f\"p-value: {p:.4f}\")\n",
        "print(f\"Degrees of freedom: {dof}\")\n",
        "print(f\"Interpretation: {'Word usage differs significantly across languages' if p < 0.05 else 'No significant difference in word usage across languages'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIYIYoatQCXQ"
      },
      "source": [
        "##[d] Common external libraries and frameworks across various domains\n",
        "\n",
        "A non-comprehensive list of useful Python libraries with documentation links for further exploration.\n",
        "\n",
        "## Data Analysis & Statistics\n",
        "- **pandas**: Data manipulation and analysis - [Documentation](https://pandas.pydata.org/docs/)\n",
        "- **NumPy**: Numerical computing with arrays and matrices - [Documentation](https://numpy.org/doc/stable/)\n",
        "- **SciPy**: Scientific computing (statistics, optimization, signal processing) - [Documentation](https://docs.scipy.org/doc/scipy/)\n",
        "- **statsmodels**: Statistical models and hypothesis testing - [Documentation](https://www.statsmodels.org/stable/index.html)\n",
        "- **pingouin**: Statistical analyses with simple API - [Documentation](https://pingouin-stats.org/)\n",
        "- **pyMC**: Bayesian statistical modeling and probabilistic machine learning - [Documentation](https://www.pymc.io/welcome.html)\n",
        "\n",
        "## Data Visualization\n",
        "- **Matplotlib**: Basic plotting library - [Documentation](https://matplotlib.org/stable/index.html)\n",
        "- **Seaborn**: Statistical data visualization based on matplotlib - [Documentation](https://seaborn.pydata.org/)\n",
        "- **Plotly**: Interactive visualizations - [Documentation](https://plotly.com/python/)\n",
        "- **Folium**: Interactive maps - [Documentation](https://python-visualization.github.io/folium/)\n",
        "\n",
        "## Machine Learning & AI\n",
        "- **scikit-learn**: Classical machine learning algorithms - [Documentation](https://scikit-learn.org/stable/)\n",
        "- **TensorFlow**: Deep learning framework by Google - [Documentation](https://www.tensorflow.org/api_docs)\n",
        "- **PyTorch**: Deep learning framework by Facebook/Meta - [Documentation](https://pytorch.org/docs/stable/index.html)\n",
        "- **Keras**: High-level neural networks API - [Documentation](https://keras.io/api/)\n",
        "- **XGBoost**: Gradient boosting implementation - [Documentation](https://xgboost.readthedocs.io/en/latest/)\n",
        "- **Hugging Face Transformers**: State-of-the-art NLP models - [Documentation](https://huggingface.co/docs/transformers/index)\n",
        "\n",
        "## Natural Language Processing\n",
        "- **NLTK**: Natural Language Toolkit for text processing - [Documentation](https://www.nltk.org/)\n",
        "- **spaCy**: Industrial-strength NLP - [Documentation](https://spacy.io/api/doc)\n",
        "- **gensim**: Topic modeling and document similarity - [Documentation](https://radimrehurek.com/gensim/)\n",
        "- **TextBlob**: Simplified text processing - [Documentation](https://textblob.readthedocs.io/en/dev/)\n",
        "\n",
        "## Web Scraping & APIs\n",
        "- **Requests**: HTTP library for API calls - [Documentation](https://requests.readthedocs.io/en/latest/)\n",
        "- **Beautiful Soup**: HTML/XML parsing library - [Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "- **Scrapy**: Web scraping framework - [Documentation](https://docs.scrapy.org/en/latest/)\n",
        "- **Selenium**: Browser automation - [Documentation](https://selenium-python.readthedocs.io/)\n",
        "\n",
        "## Social Sciences\n",
        "- **NetworkX**: Network analysis and graph theory - [Documentation](https://networkx.org/documentation/stable/)\n",
        "- **igraph**: Network analysis with a focus on efficiency - [Documentation](https://igraph.org/python/)\n",
        "- **VADER**: Sentiment analysis specifically attuned to social media - [Documentation](https://github.com/cjhutto/vaderSentiment)\n",
        "- **lifelines**: Survival analysis - [Documentation](https://lifelines.readthedocs.io/en/latest/)\n",
        "- **EconML**: Causal inference and econometrics - [Documentation](https://econml.azurewebsites.net/)\n",
        "- **scikit-learn**: For building predictive models - [Documentation](https://scikit-learn.org/stable/)\n",
        "- **NLTK**: Text analysis of survey responses - [Documentation](https://www.nltk.org/)\n",
        "\n",
        "## Physical Sciences & Engineering\n",
        "- **Astropy**: Astronomy and astrophysics - [Documentation](https://docs.astropy.org/en/stable/)\n",
        "- **Biopython**: Biological computation - [Documentation](https://biopython.org/wiki/Documentation)\n",
        "- **MDAnalysis**: Molecular dynamics analysis - [Documentation](https://www.mdanalysis.org/docs/)\n",
        "- **PyMOL**: Molecular visualization - [Documentation](https://pymol.org/dokuwiki/)\n",
        "- **RDKit**: Cheminformatics and computational chemistry - [Documentation](https://www.rdkit.org/docs/)\n",
        "- **PyCaret**: Low-code machine learning for scientific data - [Documentation](https://pycaret.gitbook.io/docs/)\n",
        "- **scikit-image**: Image processing for scientific data - [Documentation](https://scikit-image.org/docs/stable/)\n",
        "\n",
        "## Bioinformatics & Genomics\n",
        "- **Biopython**: Processing biological data - [Documentation](https://biopython.org/wiki/Documentation)\n",
        "- **scikit-bio**: Bioinformatics algorithms - [Documentation](http://scikit-bio.org/docs/latest/index.html)\n",
        "- **Pysam**: Reading/writing genomic data formats - [Documentation](https://pysam.readthedocs.io/en/latest/)\n",
        "- **Bioconductor**: (via rpy2) Genomic data analysis - [Documentation](https://bioconductor.org/)\n",
        "\n",
        "## Finance & Economics\n",
        "- **pandas-datareader**: Access to financial data - [Documentation](https://pandas-datareader.readthedocs.io/en/latest/)\n",
        "- **pyfolio**: Portfolio and risk analytics - [Documentation](https://quantopian.github.io/pyfolio/)\n",
        "- **TA-Lib**: Technical analysis library - [Documentation](https://mrjbq7.github.io/ta-lib/)\n",
        "- **Zipline**: Algorithmic trading - [Documentation](https://zipline.ml4trading.io/)\n",
        "- **arch**: ARCH models for financial time series - [Documentation](https://arch.readthedocs.io/en/latest/)\n",
        "- **statsmodels**: Time series analysis - [Documentation](https://www.statsmodels.org/stable/tsa.html)\n",
        "- **scikit-learn**: Predictive modeling for financial data - [Documentation](https://scikit-learn.org/stable/)\n",
        "\n",
        "## Geospatial Analysis\n",
        "- **GeoPandas**: Geographic data handling - [Documentation](https://geopandas.org/en/stable/)\n",
        "- **Rasterio**: Raster data access - [Documentation](https://rasterio.readthedocs.io/en/latest/)\n",
        "- **Shapely**: Manipulation of geometric objects - [Documentation](https://shapely.readthedocs.io/en/stable/)\n",
        "- **PySAL**: Spatial data science - [Documentation](https://pysal.org/pysal/)\n",
        "- **Cartopy**: Cartographic tools - [Documentation](https://scitools.org.uk/cartopy/docs/latest/)\n",
        "\n",
        "## Deep Learning Applications\n",
        "- **OpenCV** (cv2): Computer vision - [Documentation](https://docs.opencv.org/master/)\n",
        "- **librosa**: Audio analysis - [Documentation](https://librosa.org/doc/latest/index.html)\n",
        "- **Transformers**: NLP models - [Documentation](https://huggingface.co/docs/transformers/index)\n",
        "- **fastai**: Vision, text, tabular data - [Documentation](https://docs.fast.ai/)\n",
        "- **Detectron2**: Object detection - [Documentation](https://detectron2.readthedocs.io/en/latest/)\n",
        "- **Kornia**: Computer vision - [Documentation](https://kornia.readthedocs.io/en/latest/)\n",
        "\n",
        "## Web Development\n",
        "- **Django**: Full-featured web framework - [Documentation](https://docs.djangoproject.com/)\n",
        "- **Flask**: Lightweight web framework - [Documentation](https://flask.palletsprojects.com/)\n",
        "- **Streamlit**: Data apps with minimal code - [Documentation](https://docs.streamlit.io/)\n",
        "- **Dash**: Interactive analytic web applications - [Documentation](https://dash.plotly.com/)\n",
        "- **Gradio**: Create UIs for machine learning models - [Documentation](https://gradio.app/docs/)\n",
        "\n",
        "## Other Useful Libraries\n",
        "- **tqdm**: Progress bars for loops - [Documentation](https://tqdm.github.io/)\n",
        "- **pytest**: Testing framework - [Documentation](https://docs.pytest.org/)\n",
        "- **Dask**: Parallel computing - [Documentation](https://docs.dask.org/)\n",
        "- **Ray**: Distributed computing - [Documentation](https://docs.ray.io/)\n",
        "- **SQLAlchemy**: SQL toolkit and ORM - [Documentation](https://docs.sqlalchemy.org/)\n",
        "- **Pillow**: Image processing - [Documentation](https://pillow.readthedocs.io/)\n",
        "- **PyQt/PySide**: GUI development - [Documentation](https://doc.qt.io/qtforpython/)\n",
        "- **pygame**: Game development - [Documentation](https://www.pygame.org/docs/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
